# -*- coding: utf-8 -*-
"""PreTrained_BERT_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_2LDEkCzK8dec0BgSG2hT2ipp2GhPu_g
"""

"""#1"""

# import the required libraries
import torch
from transformers import BertTokenizer, BertForSequenceClassification, BertModel

"""#Data Loading"""

import csv

X_data = []
Y_data = []

with open('Every_Post.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    line_count = 0
    for row in csv_reader:
        if line_count == 0:
            print(f'Column names are {", ".join(row)}')
            line_count += 1
        else:
            post = ""
            for i in range(0, len(row) - 3):
                post += row[i]
            X_data.append(post)
            Y_data.append(row[-3])
            line_count += 1
    print(f'Processed {line_count} lines.')

"""#Data Preprocessing

https://colab.research.google.com/drive/1ywsvwO6thOVOrfagjjfuxEf6xVRxbUNO#scrollTo=cDoC24LeEv3N
"""

import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.preprocessing import LabelBinarizer

train_cnt = int(line_count * 0.8)
line_count = line_count - train_cnt
valid_test_cnt = int(line_count * 0.5)

encoder = LabelBinarizer()
Y_labeled = encoder.fit_transform(Y_data)

# Initialize the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')

import numpy as np
import keras

MAX_LENGTH = 512
# Encode the input
X_encoded = [tokenizer.encode(x, max_length=MAX_LENGTH, truncation=True, padding='max_length') for x in X_data]

# Create attention masks
attention_masks = []

# Create a mask of 1s for each token followed by 0s for padding
for seq in X_encoded:
    seq_mask = [float(i > 0) for i in seq]
    attention_masks.append(seq_mask)

from sklearn.model_selection import train_test_split

train_inputs, rem_inputs, train_labels, rem_labels = train_test_split(X_encoded, Y_labeled,
                                                                                    random_state=2018, test_size=0.2)
train_masks, rem_masks, _, _ = train_test_split(attention_masks, Y_labeled,
                                                       random_state=2018, test_size=0.2)

validation_inputs, test_inputs, validation_labels, test_labels = train_test_split(rem_inputs, rem_labels,
                                                                                    random_state=2018, test_size=0.5)
validation_masks, test_masks, _, _ = train_test_split(rem_masks, rem_labels,
                                                       random_state=2018, test_size=0.5)

train_inputs = np.asarray(train_inputs)
validation_inputs = np.asarray(validation_inputs)
test_inputs = np.asarray(test_inputs)
train_labels = np.array(train_labels)
validation_labels = np.array(validation_labels)
test_labels = np.array(test_labels)
train_masks = np.array(train_masks)
validation_masks = np.array(validation_masks)
test_masks = np.array(test_masks)

# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, 
# with an iterator the entire dataset does not need to be loaded into memory
print(len(train_inputs))
print(len(train_masks))
print(len(train_labels))

"""#Training"""

log_dir = 'tensorboard_data/tb_bert'
model_save_path = './models/bert_model.h5'

callbacks = [
    tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path, save_weights_only=True, monitor='val_loss', mode='min',
                                       save_best_only=True),
    keras.callbacks.TensorBoard(log_dir=log_dir)]

bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

print('\nBert Model', bert_model.summary())

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)

bert_model.compile(loss=loss, optimizer=optimizer, metrics=[metric])

bert_model.load_weights(model_save_path)

history = bert_model.fit([train_inputs, train_masks], train_labels, batch_size=16, epochs=3, steps_per_epoch=50,
                         validation_data=([validation_inputs, validation_masks], validation_labels),
                         callbacks=callbacks)

bert_model.save('./models/complete_model.h5')

bert_model.evaluate([test_inputs, test_masks], test_labels)

